#!/usr/bin/env python3

import os
import time
import sys
from dotenv import load_dotenv
import google.genai as genai
import google.genai.types as types  # Explicit import for clarity
# import argparse
from utils import *
from schemas import *
from system_prompt import system_prompt
from helper import call_function


args = parse_arguments()
verbose = args.verbose

load_dotenv()
api_key = os.environ.get("GEMINI_API_KEY")
client = genai.Client(api_key=api_key)

user_prompt = args.user_prompt
if verbose:
    print(f"User prompt: {user_prompt}")

messages = [
    types.Content(role="user", parts=[types.Part(text=user_prompt)]),
]

model_name = os.environ.get("GENAI_MODEL")
available_functions = types.Tool(
    function_declarations=[
        schema_get_files_info,
        schema_get_file_content,
        schema_write_file,
        schema_run_python_file,
    ]
)
config=types.GenerateContentConfig(
    tools=[available_functions], system_instruction=system_prompt
)
iterations = 50

while iterations > 0:
    iterations -= 1
    response = client.models.generate_content(
        model=model_name,
        contents=messages,
        config=config)

    # --- START OF MODIFIED BLOCK ---

    # 1. Validate the response before doing anything else.
    if not response.candidates:
        print("AGENT STOPPING: Model returned no response.")
        break

    candidate = response.candidates[0]

    # 2. Check for empty/blocked content. This is a critical safety check.
    # The 'content' can be empty if the finish_reason is not 'STOP' or 'TOOL_CALLS'.
    if not candidate.content or not candidate.content.parts:
        # If the model intended to call tools, it's okay that content is empty. We proceed.
        # If it stopped for any other reason (like SAFETY), we must halt.
        if candidate.finish_reason.name != "TOOL_CALLS":
            print(f"AGENT STOPPING: Model returned empty content. Finish Reason: {candidate.finish_reason.name}")
            break
    
    # 3. It's now safe to append the model's response (which contains its "thoughts" or function requests)
    messages.append(candidate.content)

    should_stop = False
    
    # 4. Add a "guard" to the loop. Only iterate if there are parts to iterate over.
    #    This is the direct fix for your TypeError.
    if candidate.content and candidate.content.parts:
        for part in candidate.content.parts:
            if hasattr(part, "text") and part.text:
                text = part.text.strip()
                print(text) # Always print the text part of the model's response
                if "NOTHING ELSE TO DO HERE" in text:
                    should_stop = True
                    break
    
    if should_stop:
        break

    # 5. Your parallel function call logic was already correct. No changes needed here.
    #    This block runs independently of the text-handling block above.
    if response.function_calls:
        function_response_parts = []
        for function_call in response.function_calls:
            function_result_content = call_function(function_call, verbose=verbose)
            result_part = function_result_content.parts[0]
            function_response_parts.append(result_part)

            if verbose:
                if (hasattr(result_part, "function_response") and hasattr(result_part.function_response, "response")):
                    response_dict = result_part.function_response.response
                    # Check if 'content' key exists before accessing it
                    if 'content' in response_dict:
                         # Truncate long results for cleaner logging
                         print(f"-> Function {result_part.function_response.name} result: {str(response_dict.get('content'))[:200]}...")
                    else:
                         print(f"-> Function {result_part.function_response.name} executed, no content in result.")
                else:
                    print("Could not display function response content.")

        messages.append(
            types.Content(
                role="function",
                parts=function_response_parts
            )
        )
    # --- END OF MODIFIED BLOCK ---
    
    if verbose:
        # Check for usage_metadata, as it might be missing in some error cases
        if response.usage_metadata:
            print(f"Prompt tokens: {response.usage_metadata.prompt_token_count}")
            print(f"Response tokens: {response.usage_metadata.candidates_token_count}")

    time.sleep(1)

print("\n--- Agent finished ---")
if not should_stop and response.text:
    print(response.text)